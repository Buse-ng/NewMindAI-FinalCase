import json
import sys
import os
from pathlib import Path

# Projenin kÃ¶k dizinini sys.path'e ekle
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Gerekli modÃ¼llerin importu 
from src.config.settings import PAPERS_JSON, PDF_DIR, PROCESSED_DATA_DIR
from src.chatbot.ingestion.chunker import LocalChunker
from src.chatbot.ingestion.pdf_downloader import PDFDownloader
from langchain_community.embeddings import HuggingFaceEmbeddings
import torch

def process_pdfs():
    # PDF dizini
    print(f"ğŸ“‚ PDF dizini: {PDF_DIR}")
    PDF_DIR.mkdir(parents=True, exist_ok=True)
    
    # Mevcut JSON verisi
    if not PAPERS_JSON.exists():
        print(f"âŒ JSON dosyasÄ± bulunamadÄ±: {PAPERS_JSON}")
        return
        
    print(f"ğŸ“„ JSON dosyasÄ± okunuyor: {PAPERS_JSON}")
    with open(PAPERS_JSON, 'r', encoding='utf-8') as f:
        data = json.load(f)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ–¥ï¸ KullanÄ±lan cihaz: {device}")

    # Embeddings modelini yÃ¼kle
    print(f"ğŸ”„ Embedding modeli yÃ¼kleniyor...")
    model_name = "sentence-transformers/all-MiniLM-L6-v2"
    embeddings = HuggingFaceEmbeddings(
        model_name=model_name,
        model_kwargs={'device': device}
    )
    print(f"âœ… Embedding modeli yÃ¼klendi: {model_name}")

    # PDF downloader ve chunkerÄ± oluÅŸtur
    pdf_downloader = PDFDownloader(PDF_DIR)
    chunker = LocalChunker(embeddings)

    # 'chunks' node'u yoksa oluÅŸtur
    if 'chunks' not in data['nodes']:
        data['nodes']['chunks'] = []

    # Papers.json iÃ§indeki makaleleri iÅŸle
    total_processed = 0
    total_errors = 0
    total_chunks_added = 0
    
    print(f"\nğŸš€ Ä°ÅŸlem baÅŸlatÄ±lÄ±yor: {len(data['nodes']['papers'])} makale iÅŸlenecek")
    print("=" * 60)
    
    for paper in data['nodes']['papers']:
        paper_id = paper.get('id')
        arxiv_link = paper.get('arxiv_link')
        arxiv_id = paper.get('arxiv_id')
        
        if not arxiv_link or not paper_id:
            print(f"âš ï¸ GeÃ§ersiz makale bilgisi: ID veya arxiv link eksik")
            continue
            
        print(f"\nğŸ” Ä°ÅŸleniyor: {paper['name'][:50]}... (ID: {paper_id})")
        
        try:
            # Arxiv ID'yi kullanarak PDF adÄ±nÄ± oluÅŸtur
            pdf_filename = f"{arxiv_id}.pdf" if arxiv_id else f"{paper_id}.pdf"
            
            # PDF'yi indir
            print(f"ğŸ“¥ PDF indiriliyor: {arxiv_link}")
            pdf_path = pdf_downloader.download_pdf(arxiv_link, pdf_filename)
            print(f"ğŸ“ PDF kaydedildi: {pdf_path}")
            
            # PDF'den metin Ã§Ä±kar
            print(f"ğŸ“„ Metin Ã§Ä±karÄ±lÄ±yor...")
            text, page_count = pdf_downloader.extract_text(pdf_path)
            print(f"ğŸ“Š Metin Ã§Ä±karÄ±ldÄ±: {page_count} sayfa, {len(text)} karakter")
            
            # Metni chunk'lara bÃ¶l
            print(f"âœ‚ï¸ Metin parÃ§alanÄ±yor...")
            chunks = chunker.split_text(text, page_count)
            print(f"âœ… Metin {len(chunks)} parÃ§aya bÃ¶lÃ¼ndÃ¼")
            
            # Her chunk iÃ§in embedding oluÅŸtur
            print(f"ğŸ§  Embedding'ler oluÅŸturuluyor...")
            chunk_data_list = []
            for i, chunk in enumerate(chunks):
                chunk_text = chunk
                embedding = embeddings.embed_documents([chunk_text])[0]
                chunk_id = f"chunk_{paper_id}_{i}"
                chunk_data = {'id': chunk_id, 'text': chunk_text, 'embedding': embedding, 'order': i}
                chunk_data_list.append(chunk_data)
            
            # Chunk'larÄ± data'ya ekle
            data['nodes']['chunks'].extend(chunk_data_list)
            
            # Ä°liÅŸkileri ekle
            for chunk_data in chunk_data_list:
                data['relationships'].append({
                    'from': paper_id,
                    'to': chunk_data['id'],
                    'type': 'HAS_CHUNK'
                })
            
            total_processed += 1
            total_chunks_added += len(chunk_data_list)
            print(f"ğŸ‰ Makale baÅŸarÄ±yla iÅŸlendi! {len(chunk_data_list)} chunk oluÅŸturuldu.")
                
        except Exception as e:
            print(f"âŒ PDF iÅŸlenirken hata: {e}")
            total_errors += 1
        
        print("-" * 60)

    # MetadatayÄ± gÃ¼ncelle
    data['metadata']['total_chunks'] = len(data['nodes']['chunks'])
    data['metadata']['total_relationships'] = len(data['relationships'])

    # GÃ¼ncellenmiÅŸ veriyi kaydet
    PROCESSED_DATA_DIR.mkdir(parents=True, exist_ok=True)
    
    # Ä°ÅŸlenmiÅŸ veriyi processed klasÃ¶rÃ¼ne kaydet
    processed_file = PROCESSED_DATA_DIR / 'processed_papers.json'
    with open(processed_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print("\n" + "=" * 60)
    print(f"âœ¨ Ä°ÅŸlem tamamlandÄ±!")
    print(f"ğŸ“š Toplam iÅŸlenen makale: {total_processed}")
    print(f"âŒ Toplam hata: {total_errors}")
    print(f"ğŸ§© Toplam chunk sayÄ±sÄ±: {len(data['nodes']['chunks'])}")
    print(f"ğŸ”— Toplam iliÅŸki sayÄ±sÄ±: {len(data['relationships'])}")
    print(f"ğŸ’¾ Ä°ÅŸlenmiÅŸ veriler kaydedildi: {PAPERS_JSON}")
    print(f"ğŸ’¾ Ä°ÅŸlenmiÅŸ veriler processed klasÃ¶rÃ¼ne de kaydedildi: {processed_file}")
    print("=" * 60)

if __name__ == "__main__":
    process_pdfs()
