import streamlit as st
from chatbot.utils.session import initialize_chatbot_session, chatbot_needs_reset

st.set_page_config(
    page_title="AI/ML Research Assistant",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

with open("src/chatbot/assets/styles.css") as f:
    st.markdown(f"<style>{f.read()}</style>", unsafe_allow_html=True)

st.markdown('<h1 class="main-header">ğŸ¤– AI/ML Research Assistant</h1>', unsafe_allow_html=True)
# st.markdown('<h1 class="main-header">ğŸ‘©â€ğŸ’» ğŸ‘©ğŸ’» AI/ML Research Assistant</h1>', unsafe_allow_html=True)
# st.markdown('<h1 class="main-header">ğŸ“šğŸ”  AI/ML Research Assistant</h1>', unsafe_allow_html=True)


# Sidebar AyarlarÄ±
with st.sidebar:
    st.header("âš™ï¸ Ayarlar")

    search_type = st.radio(
        "Arama Tipi:",
        ["Normal", "Reasoning", "Vector Search"],
        format_func=lambda x: {
            "Normal": "Normal Arama",
            "Reasoning": "AkÄ±llÄ± Arama (Reasoning)",
            "Vector Search": "VektÃ¶r TabanlÄ± Arama"
        }.get(x, x),
        help="Arama yapmak istediÄŸiniz yÃ¶ntemi seÃ§in"
    )
      
    # Arama tipine gÃ¶re model listesi
    if search_type == "Reasoning":
        model_options = [
            "qwen-qwq-32b",
            "deepseek-r1-distill-llama-70b"
        ]
    else:
        model_options = [
            "gpt-4.1-nano-2025-04-14",
            "llama3-70b-8192",
            "llama-3.3-70b-versatile",
            "gemma2-9b-it"
        ]

    model_name = st.selectbox(
        "Model SeÃ§imi:",
        model_options,
        help="Kullanmak istediÄŸiniz modeli seÃ§in"
    )
    #TODO: Bu kÄ±sÄ±mda search_type'a gore kontrol vardÄ±
    # model_name = st.selectbox(
    #     "Model SeÃ§imi:",
    #     ["gpt-4.1-nano-2025-04-14", "llama3-70b-8192", "llama-3.3-70b-versatile", "gemma2-9b-it"],
    #     help="Kullanmak istediÄŸiniz modeli seÃ§in"
    # )
    llm_provider = "OpenAI" if model_name == "gpt-4.1-nano-2025-04-14" else "Groq"


    temperature = st.slider(
        "Temperature:",
        min_value=0.0,
        max_value=1.0,
        value=0.1,
        step=0.1,
        help="YÃ¼ksek deÄŸerler daha yaratÄ±cÄ±, dÃ¼ÅŸÃ¼k deÄŸerler daha tutarlÄ± yanÄ±tlar Ã¼retir"
    )

    st.markdown("---")
    st.header("ğŸ’¡ Ã–rnek Sorular")
    st.subheader("ğŸ” Bu arama tipi iÃ§in Ã¶rnek sorular:")
    all_questions = {
        "Normal": [
            "AutoAgent makalesinin yazarlarÄ± kimlerdir?",
            "DocETL makalesiyle iliÅŸkili kod depolarÄ± hangileridir?",
            "Jiabin Tang'Ä±n yazdÄ±ÄŸÄ± tÃ¼m makaleleri listeler misin?",
            "MoonCast makalesinde hangi veri setleri kullanÄ±lmÄ±ÅŸ?",
            "WebDancer makalesinin Ã¶zetini verir misin?",
            "SoloSpeech makalesinin yayÄ±n tarihi nedir?",
            "Reservoir-enhanced Segment Anything Model makalesinin arXiv ve Papers With Code linkleri nedir?",
        ],
        "Reasoning": [
            "LLM ajanlarÄ±nÄ±n zorluklarÄ±yla ilgili makale bÃ¶lÃ¼mlerini bulur musun?",
            "ChartGalaxy makalesinde hangi yÃ¶ntemler (methods) kullanÄ±lmÄ±ÅŸ?",
            "ADOPT yÃ¶ntemini kullanan makaleler hangileri?",
            "Large Language Model gÃ¶revini ele alan makaleler hangileri?",
        ],
        "Vector Search": [
            "Podcast Ã¼retimiyle ilgili en alakalÄ± makale paragraflarÄ±nÄ± getir.",
            "AutoAgent hakkÄ±nda semantik olarak en yakÄ±n iÃ§eriÄŸi bul.",
            "Genel konuÅŸma sistemleriyle ilgili benzer Ã§alÄ±ÅŸmalarÄ± getir.",
        ]
    }
    
    if search_type == "Normal":
        sample_questions = (
            all_questions["Normal"] +
            all_questions["Reasoning"] +
            all_questions["Vector Search"]
        )
    else:
        sample_questions = all_questions.get(search_type, [])

    for example in sample_questions:
        if st.button(example):
            st.session_state.sample_question = example

# Session State
if "messages" not in st.session_state:
    st.session_state.messages = []
    
if "chatbot" not in st.session_state:
    initialize_chatbot_session(llm_provider, model_name, temperature, search_type)

if "initialized" not in st.session_state:
    st.session_state.initialized = True

if chatbot_needs_reset(llm_provider, model_name, temperature, search_type):
    initialize_chatbot_session(llm_provider, model_name, temperature, search_type)
    st.session_state.messages = []
    # TODO: KONTROL ET
    if search_type == "Reasoning":
        # st.info(f"Reasoning arama tipi seÃ§ildi. Otomatik olarak qwen-qwq-32b modeli kullanÄ±lacak.")
        st.info(f"AkÄ±llÄ± arama (Reasoning) seÃ§ildi. SeÃ§tiÄŸiniz {model_name} modeli kullanÄ±lacak.")
    elif search_type == "Vector Search":
        st.info(f"VektÃ¶r tabanlÄ± arama seÃ§ildi. Semantik arama iÃ§in SentenceTransformer embedding modeli kullanÄ±lacak. SeÃ§tiÄŸiniz {model_name} modeli ise sonuÃ§larÄ± iÅŸleyecektir.")
    else:
        st.info(f"LLM ayarlarÄ± deÄŸiÅŸtirildi. SaÄŸlayÄ±cÄ±: {llm_provider}, Model: {model_name}, Temperature: {temperature}")

# Chat GeÃ§miÅŸi
for msg in st.session_state.messages:
    role = msg["role"]
    content = msg["content"]
    st.markdown(f"""
    <div class='chat-message {'user-message' if role=='user' else 'bot-message'}'>
        <strong>{'ğŸ‘¤ Siz:' if role=='user' else 'ğŸ¤– Asistan:'}</strong><br>{content}
    </div>
    """, unsafe_allow_html=True)
    if role == "assistant" and "cypher_query" in msg:
        with st.expander("ğŸ” OluÅŸturulan Cypher Query"):
            st.code(msg["cypher_query"], language="cypher")

# GiriÅŸ Kutusu
user_input = st.chat_input("AI/ML araÅŸtÄ±rmalarÄ± hakkÄ±nda soru sorun...")

if "sample_question" in st.session_state:
    user_input = st.session_state.sample_question
    del st.session_state.sample_question

if user_input:
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.spinner("ğŸ¤” YanÄ±tlanÄ±yor..."):
        response = st.session_state.chatbot.get_response(user_input)
    msg = {"role": "assistant", "content": response['answer']}
    if response['cypher_query']:
        msg["cypher_query"] = response['cypher_query']
    st.session_state.messages.append(msg)
    st.rerun()